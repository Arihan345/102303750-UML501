# -*- coding: utf-8 -*-
"""Assignment-5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LZOU5bLVejW_AXjLXSQ97tE7Nnahsi0_
"""

import numpy as np
from sklearn.metrics import r2_score

np.random.seed(42)
X = np.random.randn(100, 7)
y = 5*X[:,0] + 3*X[:,1] + 2*X[:,2] + np.random.randn(100)

eps = 1e-8
X = (X - X.mean(axis=0)) / (X.std(axis=0) + eps)
y = (y - y.mean()) / (y.std() + eps)
X = np.nan_to_num(X)
y = np.nan_to_num(y)

def ridge_regression_gd(X, y, lr, lam, epochs=1000):
    m, n = X.shape
    w = np.zeros(n)
    for _ in range(epochs):
        y_pred = X.dot(w)
        grad = -(2/m)*X.T.dot(y - y_pred) + 2*lam*w
        w -= lr * grad
        w = np.clip(w, -1e6, 1e6)
    y_pred = X.dot(w)
    y_pred = np.nan_to_num(y_pred)
    cost = np.nan_to_num(np.mean((y - y_pred)**2) + lam*np.sum(w**2))
    r2 = np.nan_to_num(r2_score(y, y_pred))
    return cost, r2, w

learning_rates = [0.0001,0.001,0.01]
lambdas = [1e-5,1e-3,0.1,1,10]
best = {"cost": float("inf"), "r2": -1, "lr": None, "lam": None}

for lr in learning_rates:
    for lam in lambdas:
        cost, r2, w = ridge_regression_gd(X, y, lr, lam)
        if r2 > best["r2"]:
            best = {"cost": cost, "r2": r2, "lr": lr, "lam": lam}

print(best)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import r2_score, mean_squared_error

url = "https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Hitters.csv"
data = pd.read_csv(url)

data = data.dropna()
data = pd.get_dummies(data, drop_first=True)

X = data.drop('Salary', axis=1)
y = data['Salary']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

lr = LinearRegression().fit(X_train, y_train)
ridge = Ridge(alpha=0.5748).fit(X_train, y_train)
lasso = Lasso(alpha=0.5748).fit(X_train, y_train)

models = {'Linear': lr, 'Ridge': ridge, 'Lasso': lasso}

for name, model in models.items():
    y_pred = model.predict(X_test)
    print(name, "R2:", r2_score(y_test, y_pred), "MSE:", mean_squared_error(y_test, y_pred))

from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import RidgeCV, LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

housing = fetch_california_housing()
X, y = housing.data, housing.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

ridge_cv = RidgeCV(alphas=[0.1, 1, 10, 20]).fit(X_train, y_train)
lasso_cv = LassoCV(alphas=[0.1, 1, 10, 20]).fit(X_train, y_train)

print("Best Ridge Alpha:", ridge_cv.alpha_)
print("Ridge R2:", r2_score(y_test, ridge_cv.predict(X_test)))
print("Best Lasso Alpha:", lasso_cv.alpha_)
print("Lasso R2:", r2_score(y_test, lasso_cv.predict(X_test)))

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))