# -*- coding: utf-8 -*-
"""Assignment-4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/185hhx7RW2Z2nM3Bn9dZAOLmej9og4RQO
"""

import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re  # <-- ADDED import for regular expressions

# --- Constants ---
BASE = "https://books.toscrape.com/"
START = urljoin(BASE, "catalogue/page-1.html")
HEADERS = {"User-Agent": "Mozilla/5.0"}

STAR_MAP = {
    "One": 1, "Two": 2, "Three": 3, "Four": 4, "Five": 5
}

# --- Functions ---
def parse_page(html, data):
    """Parses a single page of books and appends the data to a list."""
    soup = BeautifulSoup(html, "html.parser")
    for art in soup.select("article.product_pod"):
        title = art.h3.a.get("title", "").strip()

        # --- MODIFIED to be more robust ---
        price_text = art.select_one(".price_color").text
        price = re.sub(r'[^0-9.]', '', price_text) # Keep only digits and the dot

        availability = art.select_one(".availability").text.strip()
        star_word = ""
        for cls in art.select_one("p.star-rating")["class"]:
            if cls in STAR_MAP:
                star_word = cls
                break
        stars = STAR_MAP.get(star_word, None)

        data.append({
            "Title": title,
            "Price": float(price), # This will now convert without error
            "Availability": "In stock" if "In stock" in availability else "Out of stock",
            "Star Rating": stars
        })

def next_link(soup):
    """Finds the link to the next page in the catalogue."""
    nxt = soup.select_one("li.next a")
    if not nxt:
        return None
    # The href is relative, so we join it with the base catalogue path
    return urljoin(BASE, "catalogue/" + nxt["href"])

def main():
    """Main function to orchestrate the scraping process."""
    data = []
    url = START
    seen = set()

    print("Starting scrape...")
    while url and url not in seen:
        print(f"Scraping: {url}")
        seen.add(url)
        r = requests.get(url, headers=HEADERS, timeout=30)
        r.raise_for_status()

        r.encoding = 'utf-8' # <-- ADDED to fix encoding issues

        parse_page(r.text, data)

        # We need the soup object to find the next link
        soup = BeautifulSoup(r.text, "html.parser")
        url = next_link(soup)

        time.sleep(0.5)  # Be polite to the server

    # Save the data to a CSV file
    df = pd.DataFrame(data, columns=["Title", "Price", "Availability", "Star Rating"])
    df.to_csv("books.csv", index=False)
    print(f"\nScrape complete. Saved {len(df)} rows to books.csv")

# --- Main Execution ---
if __name__ == "__main__":
    main()

import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time
import re

def scrape_imdb_top250():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)

    driver = webdriver.Chrome(options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    try:
        url = "https://www.imdb.com/chart/top/"
        print(f"Navigating to {url}")
        driver.get(url)

        wait = WebDriverWait(driver, 20)
        time.sleep(5)

        movies_data = []

        selectors_to_try = [
            ".ipc-metadata-list-summary-item",
            ".titleColumn",
            ".cli-item",
            "li[data-testid='title-list-item']",
            ".ipc-title-link-wrapper",
            "tr[data-testid='watchlist-row']"
        ]

        movie_elements = None
        for selector in selectors_to_try:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    movie_elements = elements
                    print(f"Found {len(elements)} elements using selector: {selector}")
                    break
            except:
                continue

        if not movie_elements:
            print("Trying alternative approach - looking for any movie containers...")
            movie_elements = driver.find_elements(By.XPATH, "//*[contains(@class, 'title') or contains(@class, 'movie') or contains(@class, 'film')]")

        if not movie_elements:
            print("Extracting from page source...")
            page_source = driver.page_source

            title_pattern = r'<a[^>]*href="/title/tt\d+/"[^>]*>([^<]+)</a>'
            year_pattern = r'\((\d{4})\)'
            rating_pattern = r'(\d\.\d)'

            titles = re.findall(title_pattern, page_source)
            years = re.findall(year_pattern, page_source)
            ratings = re.findall(rating_pattern, page_source)

            for i, (title, year, rating) in enumerate(zip(titles[:250], years[:250], ratings[:250])):
                movies_data.append({
                    'Rank': i + 1,
                    'Movie Title': title.strip(),
                    'Year of Release': year,
                    'IMDB Rating': float(rating)
                })
                print(f"Scraped: {i+1}. {title} ({year}) - {rating}")

        else:
            for i, element in enumerate(movie_elements[:250]):
                try:
                    rank = i + 1

                    title = "N/A"
                    title_selectors = [
                        ".ipc-title a h3",
                        ".ipc-title h3",
                        ".titleColumn a",
                        ".ipc-title-link-wrapper h3",
                        "h3",
                        "a[href*='/title/']"
                    ]

                    for sel in title_selectors:
                        try:
                            title_elem = element.find_element(By.CSS_SELECTOR, sel)
                            title = title_elem.text.strip()
                            title = re.sub(r'^\d+\.\s*', '', title)
                            if title and title != "N/A":
                                break
                        except:
                            continue

                    year = "N/A"
                    year_selectors = [
                        ".sc-300a8231-7",
                        ".secondaryInfo",
                        ".ipc-inline-list__item",
                        "[data-testid='title-metadata-item']"
                    ]

                    for sel in year_selectors:
                        try:
                            year_elem = element.find_element(By.CSS_SELECTOR, sel)
                            year_text = year_elem.text
                            year_match = re.search(r'\((\d{4})\)', year_text)
                            if year_match:
                                year = year_match.group(1)
                                break
                        except:
                            continue

                    rating = "N/A"
                    rating_selectors = [
                        ".ipc-rating-star--rating",
                        ".ratingValue strong",
                        "[data-testid='rating-button'] span",
                        ".ipc-rating-star"
                    ]

                    for sel in rating_selectors:
                        try:
                            rating_elem = element.find_element(By.CSS_SELECTOR, sel)
                            rating_text = rating_elem.text.strip()
                            rating_match = re.search(r'(\d\.\d)', rating_text)
                            if rating_match:
                                rating = rating_match.group(1)
                                break
                        except:
                            continue

                    if title != "N/A":
                        movies_data.append({
                            'Rank': rank,
                            'Movie Title': title,
                            'Year of Release': year,
                            'IMDB Rating': float(rating) if rating != "N/A" else 0.0
                        })

                        print(f"Scraped: {rank}. {title} ({year}) - {rating}")

                except Exception as e:
                    print(f"Error scraping movie {i+1}: {str(e)}")
                    continue

        if movies_data:
            df = pd.DataFrame(movies_data)
            csv_filename = 'imdb_top250.csv'
            df.to_csv(csv_filename, index=False)

            print(f"\nSuccessfully scraped {len(movies_data)} movies")
            print(f"Data saved to {csv_filename}")
            print("\nFirst 5 movies:")
            print(df.head())
            return df
        else:
            print("No movies data could be scraped")
            return None

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None

    finally:
        driver.quit()

if __name__ == "__main__":
    movies_df = scrape_imdb_top250()

    if movies_df is not None:
        print(f"\nTotal movies scraped: {len(movies_df)}")
        print(f"Columns: {list(movies_df.columns)}")

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://www.timeanddate.com/weather/india/new-delhi/historic"
headers = {"User-Agent": "Mozilla/5.0"}
res = requests.get(url, headers=headers)
soup = BeautifulSoup(res.text, "html.parser")

table = soup.find("table", id="wt-his")
if table:
    data = []
    for tr in table.find("tbody").find_all("tr"):
        time_label = tr.find("th").get_text(strip=True)
        tds = tr.find_all("td")
        data.append({
            "Time": time_label,
            "Temperature": tds[1].get_text(strip=True),
            "Weather": tds[2].get_text(strip=True),
            "Wind": tds[3].get_text(strip=True),
            "Humidity": tds[5].get_text(strip=True),
            "Barometer": tds[6].get_text(strip=True),
            "Visibility": tds[7].get_text(strip=True),
        })
    df = pd.DataFrame(data)
    df.to_csv("historic_weather_delhi.csv", index=False)
    print("Scraped historical weather:", len(df), "records")
else:
    print("Historic weather table not found!")